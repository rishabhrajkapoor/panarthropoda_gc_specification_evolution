{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482b488e-10b6-4ee4-af78-b1542ccbaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90443f39-8119-4dac-90aa-34f4386e119b",
   "metadata": {},
   "source": [
    "## Download and prep protein datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5689ed8-2c4f-470d-847a-7bdab618380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a dataframe with accessions of all genomes/transcriptomes \n",
    "df=pd.read_csv(\"phylogenetic_data_with_substitutions.tsv\",sep=\"\\t\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130959d0-0c57-42f3-9057-4bb2ddfc2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##download transcriptome shotgun assemblies (TSAs)\n",
    "!mkdir TSA\n",
    "td=list(df[df.data_type=='TSA'].index)\n",
    "for prefix in td:\n",
    "    prefix=prefix.split(\".\")[0]\n",
    "    try:\n",
    "        ##extract prefixes for indexing into sra traces \n",
    "        n1=prefix[0:2]\n",
    "        n2=prefix[2:4]\n",
    "        ##obtain the wgs number\n",
    "        a=subprocess.check_output([\"curl\",\"-s\",f\"https://locate.ncbi.nlm.nih.gov/sdl/2/retrieve?acc={prefix}000000\"])\n",
    "        wgs_number=str(a).split('\"link\":')[1].split(\"traces/\")[1].split(\"/WGS/\")[0]\n",
    "        address=f\"https://sra-download.ncbi.nlm.nih.gov/traces/{wgs_number}/wgs_aux/{n1}/{n2}/{prefix}/{prefix}.1.fsa_nt.gz\"\n",
    "        a=subprocess.check_output([\"wget\",address])\n",
    "        subprocess.run(f\"gunzip -c {prefix}.1.fsa_nt.gz > TSA/{prefix}.1.fasta\", shell=True)\n",
    "        !rm \"$prefix\".1.fsa_nt.gz\n",
    "    except:\n",
    "        f=open('failed_tsa_download.fasta','a')\n",
    "        f.write(f'{prefix}\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd176b1-fece-41bb-83de-dbd4ce12d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run transdecoder to get protein predictions\n",
    "for p in td:\n",
    "    !sh transdecoder.sh \"$p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a07503-3ba7-4d63-9e65-56bdc63706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##download unannotated genomes \n",
    "td=list(df[df.data_type=='unannotated_genome'].index)\n",
    "\n",
    "for g in td:\n",
    "    !sh download_genome_only.sh $g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8f1e99-40b1-4fb4-b6de-d80ea994a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "##download annotated genomes with protein predictitons\n",
    "td=list(df[df.data_type=='annotated_genome'].index)\n",
    "for g in td:\n",
    "    !sh download_prot_genome.sh $g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c6dac6a-59c7-4d9c-86c9-0406ee0a4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scripts to extract the longest protein isoform per gene (written by RK with ChatGPT)\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "\n",
    "def parse_gff3(gff_path):\n",
    "    \"\"\"\n",
    "    Parse a GFF3 and return a dict mapping transcript IDs -> gene IDs.\n",
    "    \"\"\"\n",
    "    trans2gene = {}\n",
    "    with open(gff_path) as gff:\n",
    "        for line in gff:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            cols = line.strip().split('\\t')\n",
    "            if len(cols) < 9 or cols[2] != 'CDS':\n",
    "                continue\n",
    "            attrs = dict(kv.split('=', 1) for kv in cols[8].split(';') if '=' in kv)\n",
    "            \n",
    "            tid = attrs.get('ID')\n",
    "            \n",
    "            gid = attrs.get('gene')\n",
    "            \n",
    "            if not gid:\n",
    "                 gid = attrs.get('locus_tag')\n",
    "\n",
    "            if tid and gid:\n",
    "                trans2gene[tid.split(\"cds-\")[1]] = gid\n",
    "    return trans2gene\n",
    "\n",
    "def longest_per_gene(faa_path, trans2gene):\n",
    "    \"\"\"\n",
    "    Group protein records by gene and return the longest sequence per gene.\n",
    "    \"\"\"\n",
    "    best = {}\n",
    "    for rec in SeqIO.parse(faa_path, \"fasta\"):\n",
    "        tid = rec.id.split()[0]\n",
    "        gid = trans2gene.get(tid, tid)\n",
    "        if gid not in best or len(rec.seq) > len(best[gid].seq):\n",
    "            best[gid] = rec\n",
    "    return best\n",
    "\n",
    "def write_longest(out_path, gene2rec):\n",
    "    \"\"\"\n",
    "    Write the longest SeqRecords to a FASTA, appending gene ID to headers.\n",
    "    \"\"\"\n",
    "    with open(out_path, 'w') as out:\n",
    "        for gid, rec in gene2rec.items():\n",
    "            rec.id = f\"{rec.id}\"\n",
    "            rec.description = ''\n",
    "            SeqIO.write(rec, out, 'fasta')\n",
    "\n",
    "def extract_longest_protein_per_gene(x):\n",
    "    \"\"\"\n",
    "    Extract the longest protein per gene from all .faa/.fa files in the given directory,\n",
    "    using the single GFF3 annotation in that directory.\n",
    "    \"\"\"\n",
    "    directory=f'ncbi_dataset/data/{x}'\n",
    "    # find GFF3 file\n",
    "    gff_files = glob.glob(os.path.join(directory, \"*.gff3\")) + \\\n",
    "                glob.glob(os.path.join(directory, \"*.gff\"))\n",
    "    if not gff_files:\n",
    "        sys.exit(\"Error: no GFF(.gff3) found in directory.\")\n",
    "    if len(gff_files) > 1:\n",
    "        sys.exit(\"Error: more than one GFF(.gff3) found; please keep only one per run.\")\n",
    "    gff_path = gff_files[0]\n",
    "    print(f\"Using GFF: {gff_path}\")\n",
    "\n",
    "    # build transcript->gene map\n",
    "    trans2gene = parse_gff3(gff_path)\n",
    "    if not trans2gene:\n",
    "        sys.exit(\"Error: no mRNA→gene mappings found in GFF. Check feature types/attributes.\")\n",
    "\n",
    "    faa=f'ncbi_dataset/data/{x}/protein.faa'\n",
    "    print(f\"Processing {faa} ...\")\n",
    "    gene2rec = longest_per_gene(faa, trans2gene)\n",
    "    base = os.path.splitext(os.path.basename(faa))[0]\n",
    "    out_faa = os.path.join(directory, f\"{base}_longest.faa\")\n",
    "    write_longest(out_faa, gene2rec)\n",
    "    print(f\"  → wrote {len(gene2rec)} records to {out_faa}\")\n",
    "for g in td:\n",
    "    extract_longest_protein_per_gene(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14088a8-8ff0-4e22-b0e5-4f5d6601793c",
   "metadata": {},
   "source": [
    "## Run BUSCO to extract orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c532f0-d2c3-4547-8a03-346ad106582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##unzip busco gene set downloaded from https://busco-data.ezlab.org/v5/data/lineages/arthropoda_odb12.2025-07-01.tar.gz\n",
    "!tar -xvzf arthropoda_odb12.2025-04-11.tar.gz\n",
    "!mkdir busco_downloads/arthropoda_odb12\n",
    "##move into expected directory to run BUSCO\n",
    "!mv arthropoda_odb12 busco_downloads/lineages/arthropoda_odb12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cb1c9e4-1d30-4b3f-b6d2-4d355fb0470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p BUSCO_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e86df4-dd6b-4ba6-9ff2-9622c32c0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run BUSCO on each dataset type \n",
    "td=list(df[df.data_type=='TSA'].index)\n",
    "for x in td:\n",
    "    !sh run_busco_prot_tsa.sh \"$x\"\n",
    "td=list(df[df.data_type=='annotated_genome'].index)\n",
    "for x in td:\n",
    "    !sh run_busco_annotated_genome.sh \"$x\"\n",
    "td=list(df[df.data_type=='unannotated_genome'].index)\n",
    "for x in td:\n",
    "    !sh run_busco_un_annotated_genome.sh \"$x\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6abad3-e8c2-41fb-be3a-ae23c3b97557",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add BUSCO completenes (single copy, complete) to df \n",
    "for x in list(df.index):\n",
    "    f=open(f\"BUSCO_outputs/{x}/short_summary.specific.arthropoda_odb12.{x}.txt\",\"r\").readlines()\n",
    "    b=float([x for x in f if \"C:\" in x][0].split(\"S:\")[1].split(\"%\")[0])\n",
    "    df.loc[x,'BUSCO_complete_single_copy']=b\n",
    "df.to_csv(\"phylogenetic_data_with_substitutions.tsv\",sep=\"\\t\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72647507-f4ec-41e9-ae9b-c42b94f7b4ea",
   "metadata": {},
   "source": [
    "## Run BUSCOphylogenomics\n",
    "Obtain conda environment from https://github.com/jamiemcg/BUSCO_phylogenomics. This pipeline generates MUSCLE alignments from BUSCO orthologs in BUSCO_outputs, trims them with trimAl, then concatenates to make a supermatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "618e1bfb-d030-43e0-b923-6a3eedb4735d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sbatch run_busco_phylogenomics.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e76d4-835c-4f41-bcef-2ce323fd88b5",
   "metadata": {},
   "source": [
    "## Make constraint tree for IQtree search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614226c3-fbc6-4115-a86e-157b1be31de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"phylogenetic_data_with_substitutions.tsv\",sep=\"\\t\",index_col=0)\n",
    "\n",
    "##load species-level phylogram\n",
    "from ete3 import Tree\n",
    "t=Tree('constraint_trees/all_combined_species.nwk',format=9)\n",
    "\n",
    "## rename leaves of the tree with the taxonomic rank at which a substitution was made (if any)\n",
    "n=0\n",
    "keep=[]\n",
    "done=set()\n",
    "nmap={}\n",
    "for x in t:\n",
    "    if x.name.replace(\"_\",\" \") in set(df.species):\n",
    "        keep.append(x)\n",
    "        nmap[df[df.species==x.name.replace(\"_\",\" \")].index.values[0]]=x.name.replace(\"_\",\" \")\n",
    "    else:\n",
    "        dfs = df[df.subs.astype(str).str.contains(x.name.replace(\"_\",\" \"))]\n",
    "        if dfs.shape[0]>0:\n",
    "            if dfs.order.values[0]=='Amblypygi' or dfs.order.values[0]=='Pseudoscorpiones' or dfs.order.values=='Parachela':\n",
    "                x.name=dfs.order.values[0]\n",
    "                \n",
    "                if x.name not in done:\n",
    "                    keep.append(x)\n",
    "                    done.add(x.name)\n",
    "                    nmap[dfs.index.values[0]]=x.name\n",
    "            else:\n",
    "                x.name=dfs.family.values[0]\n",
    "                if x.name not in done:\n",
    "                    keep.append(x)\n",
    "                    done.add(x.name)\n",
    "                    nmap[dfs.index.values[0]]=x.name\n",
    "##prune tree to eliminate species not in species-level constraint tree\n",
    "##also limits representatives of clades with substitutions to at most one\n",
    "t.prune(keep)\n",
    "t.ladderize()\n",
    "\n",
    "## add a column to phylogenetic_data_with_substitutions for updated node names \n",
    "for index, row in df.iterrows():\n",
    "    df.loc[index,'iq_tree_label']=nmap[index].replace(\" \",\"_\").strip()\n",
    "df.to_csv(\"phylogenetic_data_with_substitutions.tsv\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "##output newick for constrained IQtree search\n",
    "t.write(outfile='BUSCO_py/constraint_tree_iqtree_species.tree',format=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b67beaa-74af-4957-910f-e82a8f1d660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename taxa names to remove spaces for iq-tree compatibility\n",
    "from Bio import AlignIO\n",
    "nmap={x:nmap[x].replace(\" \",\"_\") for x in nmap}\n",
    "phylip_in   = \"BUSCO_py/supermatrix/SUPERMATRIX.phylip\"       # original file\n",
    "phylip_out  = \"BUSCO_py/supermatrix/SUPERMATRIX.phy\"\n",
    "\n",
    "\n",
    "# ── read the alignment ──────────────────────────────────────────────\n",
    "alignment = AlignIO.read(phylip_in, \"phylip-relaxed\")  # or \"phylip\"\n",
    "\n",
    "# ── rename the taxa ────────────────────────────────────────────────\n",
    "for record in alignment:\n",
    "    if record.id in nmap:\n",
    "        new_id            = nmap[record.id]\n",
    "        record.id         = new_id              # first field in PHYLIP\n",
    "        record.name       = new_id              # SeqRecord secondary name\n",
    "        record.description = new_id             # unused in PHYLIP\n",
    "    # else leave the original ID unchanged (or raise an error)\n",
    "\n",
    "# ── write the renamed alignment ────────────────────────────────────\n",
    "AlignIO.write(alignment, phylip_out, \"phylip-relaxed\")    # or \"phylip\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f74a1-920c-4e18-ba35-9dc0350f56a1",
   "metadata": {},
   "source": [
    "## Run IQ-tree constrained tree search\n",
    "Uses singularity container of iqtree:2.4.0 to perform a constrained maximum likelihood tree search with a constraint tree provided by BUSCO_py/constraint_tree_iqtree_species.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "670044c1-812e-4e10-bc39-2fda22197dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch run_iqtree_constrained_search.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f9b32-5ac1-4642-b032-47006b989565",
   "metadata": {},
   "source": [
    "## Run IQ2MC pipeline for fossil calibrations with MCMCtree\n",
    "uses a locally installed version of iqtree 3 to produce hessian matrix for approximate likelihood computatoins with MCMCtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77130a29-1962-4044-83a9-7257b78d50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataframe of fossil constraints, scale to units of 100 million years \n",
    "dfc=pd.read_csv(\"BUSCO_py/supermatrix/Fossil_constraints.tsv\",sep='\\t',index_col=0)\n",
    "dfc.Min=dfc.Min/100\n",
    "dfc.Max=dfc.Max/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8187237-24c2-47d7-b47f-00b2f9d005ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home11/rkapoor/.conda/envs/rishabh/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import ete3\n",
    "from ete3 import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24178bd8-eefd-41d7-a0cf-ffe0d2fa8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add fossil constraints to tree using MCMCtree-compatible notation\n",
    "##Load treefile output of IQtree manually rooted with tardigrades as the outgroup\n",
    "t=Tree(\"BUSCO_py/supermatrix/rooted_constrained_optimized.treefile\")\n",
    "f=open('BUSCO_py/supermatrix/fossil_rooted_constrained_optimized.treefile','r')\n",
    "l=f.readlines()[0].replace(\"[&&NHX:bound=\",\"\").replace(\"]\",\"\")\n",
    "##add root constraints for panarthropoda from howard et al 2022\n",
    "l.replace(';',\" '>528.82<636.1';\")\n",
    "l.replace('6.361000000000001','6.361')\n",
    "f=open('BUSCO_py/supermatrix/fossil_rooted_constrained_optimized.treefile','w')\n",
    "##note that calibrations at the root were manually added \n",
    "f.write(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f77b3a-c7e9-4cbe-a12c-cb874da8f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run local version of iqtree3 for MCMC prep\n",
    "!sh iqtree3_iq2mcmc.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ec171-fae3-4229-a35b-4f289b92a805",
   "metadata": {},
   "source": [
    "## Instructions for running MCMCtree\n",
    "Once iqtree3_iq2mcmc.sh finishes running, change name of hessian output file \"BUSCO_py/supermatrix/mcmc_prep_no_partition.mcmctree.hessian\" to USCO_py/supermatrix/in.BV to use with MCMCtree\n",
    "make two separate .ctl files \"BUSCO_py/supermatrix/run1R_mcmc_prep_no_partition.mcmctree.ctl\" and \"BUSCO_py/supermatrix/run2R_mcmc_prep_no_partition.mcmctree.ctl\" from IQtree output\n",
    "Run each with a local version of PAML (https://github.com/abacus-gene/paml, version 4.10.9) using mcmctree \"BUSCO_py/supermatrix/run1R_mcmc_prep_no_partition.mcmctree.ctl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96deb672-88ac-4255-9920-867c6e7ff3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r BUSCO_py panarthropoda_gc_specification_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8589993-9124-4eb9-b7e0-b8a8ed9b08cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh",
   "language": "python",
   "name": "rishabh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
